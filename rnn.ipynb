{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Heykd9lIeDU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWiplXOzI5_B"
      },
      "source": [
        "Select for best 5 combinations for rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "93PuOKL8IvVs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# === Excel einlesen ===\n",
        "file_path = r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# === Zielvariable & CNN-geeignete Feature-Kandidaten ===\n",
        "target_col = \"_MKT\"\n",
        "allowed_features = [\n",
        "    \"EMP\",       # Besch√§ftigungstrend\n",
        "    \"GDP\",       # Wirtschaftswachstum\n",
        "    \"UN\",        # Arbeitslosigkeit\n",
        "    \"CPI\",       # Inflation\n",
        "    \"M2\",        # Geldmengenwachstum\n",
        "    \"Y02\",       # Kurzfristige Rendite\n",
        "    \"Y10\",       # Langfristige Rendite\n",
        "    \"STP\",       # Steilheit Zinskurve\n",
        "    \"IR\",        # Nominalzins\n",
        "    \"RR\",        # Realzins\n",
        "    \"MOV\",       # Volatilit√§t\n",
        "    \"NYF\",       # New York Fed Index\n",
        "    \"_TY\",       # Treasury Markt\n",
        "    \"_OIL\",      # √ñlpreis\n",
        "    \"_DXY\",      # Dollar Index\n",
        "    \"_LCP\",      # Large Cap Index\n",
        "    \"_AU\"        # Goldpreis\n",
        "]\n",
        "\n",
        "# === Datum verarbeiten ===\n",
        "if \"Date\" in df.columns:\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.sort_values(\"Date\")\n",
        "    df = df.set_index(\"Date\")\n",
        "\n",
        "# === Nur numerische Daten & Normalisieren ===\n",
        "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
        "\n",
        "# === Split: 15% Training, Rest Validierung ===\n",
        "split_index = int(len(df_scaled) * 0.15)\n",
        "train_df = df_scaled[:split_index]\n",
        "val_df = df_scaled[split_index:]\n",
        "\n",
        "# === Zeitreihen-Daten generieren ===\n",
        "def create_dataset(X, y, seq_len=5):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i + seq_len])\n",
        "        ys.append(y[i + seq_len])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# === RNN (LSTM) testen mit allen 3er-Kombinationen ===\n",
        "results = []\n",
        "for combo in combinations(allowed_features, 3):\n",
        "    combo = list(combo)\n",
        "    try:\n",
        "        X_train, y_train = create_dataset(train_df[combo].values, train_df[target_col].values)\n",
        "        X_val, y_val = create_dataset(val_df[combo].values, val_df[target_col].values)\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            epochs=50,\n",
        "                            batch_size=16,\n",
        "                            verbose=0,\n",
        "                            callbacks=[early_stop])\n",
        "\n",
        "        val_loss = min(history.history[\"val_loss\"])\n",
        "        results.append((combo, val_loss))\n",
        "        print(f\"‚úÖ Getestet (LSTM): {combo} | val_loss: {val_loss:.5f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Fehler bei Kombination {combo}: {str(e)}\")\n",
        "\n",
        "# === Beste 5 Kombinationen anzeigen ===\n",
        "results.sort(key=lambda x: x[1])\n",
        "print(\"\\nüèÜ Beste 5 Kombinationen mit genau 3 Features (LSTM):\")\n",
        "for i, (combo, loss) in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. {combo} ‚ûû val_loss: {loss:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "besz results  \n",
        "1. ['GDP', '_TY', '_DXY'] ‚ûû val_loss: 0.06024\n",
        "2. ['CPI', '_TY', '_LCP'] ‚ûû val_loss: 0.06063\n",
        "3. ['UN', 'Y02', '_TY'] ‚ûû val_loss: 0.06120\n",
        "4. ['_TY', '_DXY', '_LCP'] ‚ûû val_loss: 0.06159\n",
        "5. ['Y02', '_TY', '_DXY'] ‚ûû val_loss: 0.06295\n"
      ],
      "metadata": {
        "id": "cuJKDRVcRm34"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA4TsXfNI3Bg"
      },
      "source": [
        "best arhitecture for each result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "0D25t1SrJHo_",
        "outputId": "3608a9f2-3459-41a9-873b-6c7cf822f7f3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Kopie von market_data.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3196174993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# --- Daten einlesen ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/Kopie von market_data.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# --- Datumsspalte erkennen ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Kopie von market_data.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# === Reproduzierbarkeit ===\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# === Daten einlesen ===\n",
        "df = pd.read_excel(r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\")\n",
        "df = df.dropna()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "target_col = \"_MKT\"\n",
        "\n",
        "# === Feature-Kombinationen ===\n",
        "combinations_to_test = [\n",
        "    ['GDP', '_TY', '_DXY'],       # ‚ûû val_loss: 0.06024\n",
        "    ['CPI', '_TY', '_LCP'],       # ‚ûû val_loss: 0.06063\n",
        "    ['UN', 'Y02', '_TY'],         # ‚ûû val_loss: 0.06120\n",
        "    ['_TY', '_DXY', '_LCP'],      # ‚ûû val_loss: 0.06159\n",
        "    ['Y02', '_TY', '_DXY']        # ‚ûû val_loss: 0.06295\n",
        "]\n",
        "\n",
        "\n",
        "# === WindowGenerator ===\n",
        "class WindowGenerator():\n",
        "    def __init__(self, input_width, label_width, shift, input_columns=None, label_columns=None, df_train=None):\n",
        "        self.label_columns = label_columns\n",
        "        self.input_columns = input_columns\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "        self.total_window_size = input_width + shift\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "\n",
        "        if df_train is not None:\n",
        "            self.train_input_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "            self.train_label_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "\n",
        "    def split_window(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.label_start:, :]\n",
        "        if self.input_columns:\n",
        "            inputs = tf.stack([inputs[:, :, self.train_input_indices[name]] for name in self.input_columns], axis=-1)\n",
        "        if self.label_columns:\n",
        "            labels = tf.stack([labels[:, :, self.train_label_indices[name]] for name in self.label_columns], axis=-1)\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, shuffle=False, batchsize=64):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            sampling_rate=1,\n",
        "            shuffle=shuffle,\n",
        "            batch_size=batchsize\n",
        "        )\n",
        "        return ds.map(self.split_window)\n",
        "\n",
        "# === Hyperparameter-Space erweitern ===\n",
        "hyperparams = list(product(\n",
        "    [10, 20, 30, 45],\n",
        "    [(32, 64, 128), (64, 64, 64), (128, 64, 32)],\n",
        "    [(0.1, 0.3), (0.2, 0.4), (0.3, 0.5)],\n",
        "    [32, 64, 128, 256]\n",
        "))\n",
        "hyperparams = random.sample(hyperparams, 40)\n",
        "\n",
        "# === Ergebnisliste ===\n",
        "final_results = []\n",
        "\n",
        "# === Loop √ºber Feature-Kombinationen ===\n",
        "for features in combinations_to_test:\n",
        "    print(f\"\\nüß™ Testing: {features}\")\n",
        "    selected_cols = features + [target_col]\n",
        "    data = df_numeric[selected_cols].copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=selected_cols)\n",
        "\n",
        "    split = int(len(data_scaled) * 0.8)\n",
        "    train_df = data_scaled[:split]\n",
        "    val_df = data_scaled[split:]\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_corr = -1\n",
        "    best_sharpe = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    for input_width, units, drops, dense in hyperparams:\n",
        "        window = WindowGenerator(input_width=input_width, label_width=1, shift=1,\n",
        "                                 input_columns=features, label_columns=[target_col], df_train=train_df)\n",
        "        train_data = window.make_dataset(train_df, shuffle=True)\n",
        "        val_data = window.make_dataset(val_df)\n",
        "\n",
        "        model = Sequential([\n",
        "            LSTM(units=units[0], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[0]),\n",
        "            LSTM(units=units[1], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[1]),\n",
        "            LSTM(units=units[2], return_sequences=False),\n",
        "            BatchNormalization(),\n",
        "            Dense(dense, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse')\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=50,\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        y_pred_val = model.predict(val_data)\n",
        "        y_true_val = np.concatenate([y for x, y in val_data], axis=0)\n",
        "\n",
        "        if y_pred_val.ndim == 3:\n",
        "            y_pred_val = y_pred_val[:, -1, :]\n",
        "        if y_true_val.ndim == 3:\n",
        "            y_true_val = y_true_val[:, -1, :]\n",
        "\n",
        "        corr, _ = pearsonr(np.ravel(y_true_val), np.ravel(y_pred_val))\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "        # Lineare Regression zur Korrektur\n",
        "        reg = LinearRegression().fit(y_pred_val.reshape(-1, 1), y_true_val.reshape(-1, 1))\n",
        "        y_pred_corrected = reg.predict(y_pred_val.reshape(-1, 1))\n",
        "\n",
        "        # Sharpe Ratio berechnen\n",
        "        returns = y_true_val[1:] - y_true_val[:-1]\n",
        "        position = np.sign(y_pred_corrected[1:] - y_pred_corrected[:-1])\n",
        "        strategy_returns = position * returns\n",
        "        sharpe_ratio = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-6)\n",
        "\n",
        "        if sharpe_ratio > best_sharpe or (sharpe_ratio == best_sharpe and corr > best_corr):\n",
        "            best_loss = val_loss\n",
        "            best_corr = corr\n",
        "            best_sharpe = sharpe_ratio\n",
        "            best_config = (input_width, units, drops, dense)\n",
        "\n",
        "    print(f\"‚úÖ Best Config: {best_config} | loss: {best_loss:.5f} | corr: {best_corr:.3f} | Sharpe: {best_sharpe:.3f}\")\n",
        "    final_results.append((features, best_loss, best_corr, best_config, best_sharpe))\n",
        "\n",
        "# === Ergebnisse anzeigen (Sharpe-basiert sortiert) ===\n",
        "final_results.sort(key=lambda x: (x[4], x[2]), reverse=True)\n",
        "print(\"\\nüèÅ Best Feature Combinations:\")\n",
        "for i, (feat, loss, corr, cfg, sharpe) in enumerate(final_results, 1):\n",
        "    print(f\"{i}. {feat} ‚ûî val_loss: {loss:.5f} | corr: {corr:.3f} | Sharpe: {sharpe:.3f} | config: {cfg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ['_TY', '_DXY', '_LCP'] ‚ûî val_loss: 0.11024 | corr: 0.694 | Sharpe: 0.127 | config: (20, (128, 64, 32), (0.1, 0.3), 128)\n",
        "2. ['CPI', '_TY', '_LCP'] ‚ûî val_loss: 0.25088 | corr: 0.876 | Sharpe: 0.126 | config: (20, (64, 64, 64), (0.2, 0.4), 256)\n",
        "3. ['GDP', '_TY', '_DXY'] ‚ûî val_loss: 0.39701 | corr: 0.347 | Sharpe: 0.099 | config: (20, (32, 64, 128), (0.2, 0.4), 256)\n",
        "4. ['UN', 'Y02', '_TY'] ‚ûî val_loss: 0.24832 | corr: 0.384 | Sharpe: 0.087 | config: (30, (128, 64, 32), (0.2, 0.4), 256)\n",
        "5. ['Y02', '_TY', '_DXY'] ‚ûî val_loss: 0.31353 | corr: 0.523 | Sharpe: 0.061 | config: (30, (128, 64, 32), (0.1, 0.3), 64)\n",
        "\n",
        "only number 1 and 2 are relevant because of sharp ratio and korelation\n",
        "\n",
        "now thind the best traiding strategy for 1 and 2"
      ],
      "metadata": {
        "id": "nbOUe6pVR_t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_excel(r\"C:\\\\Users\\\\41799\\\\Desktop\\\\Kopie von market_data.xlsx\")\n",
        "df = df.dropna()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Parameters\n",
        "target_col = '_MKT'\n",
        "features = ['_TY', '_DXY', '_LCP']\n",
        "hyperparam_space = [\n",
        "    (20, (128, 64, 32), (0.1, 0.3), 128),\n",
        "    (20, (64, 64, 64), (0.2, 0.4), 256),\n",
        "    (15, (32, 64, 128), (0.2, 0.4), 256),\n",
        "    (10, (32, 32, 32), (0.1, 0.2), 64)\n",
        "]\n",
        "\n",
        "class WindowGenerator:\n",
        "    def __init__(self, input_width, label_width, shift, input_columns, label_columns, df_train):\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "        self.total_window_size = input_width + shift\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "        self.input_columns = input_columns\n",
        "        self.label_columns = label_columns\n",
        "        self.train_indices = {col: i for i, col in enumerate(df_train.columns)}\n",
        "\n",
        "    def split_window(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.label_start:, :]\n",
        "        inputs = tf.stack([inputs[:, :, self.train_indices[col]] for col in self.input_columns], axis=-1)\n",
        "        labels = tf.stack([labels[:, :, self.train_indices[col]] for col in self.label_columns], axis=-1)\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, batchsize=64):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            sampling_rate=1,\n",
        "            shuffle=False,\n",
        "            batch_size=batchsize)\n",
        "        return ds.map(self.split_window)\n",
        "\n",
        "# Strategy evaluation\n",
        "def evaluate_strategy(y_true, y_pred):\n",
        "    reg = LinearRegression().fit(y_pred, y_true)\n",
        "    y_pred_corr = reg.predict(y_pred)\n",
        "    returns = y_true[1:] - y_true[:-1]\n",
        "    strategies = {\n",
        "        'sign': np.sign(y_pred_corr[1:] - y_pred_corr[:-1]),\n",
        "        'tanh': np.tanh(y_pred_corr[1:] - y_pred_corr[:-1]),\n",
        "        'adaptive': np.clip((y_pred_corr[1:] - y_pred_corr[:-1]) * 5, -1, 1)\n",
        "    }\n",
        "    sharpes = {k: np.mean(v * returns) / (np.std(v * returns) + 1e-6) for k, v in strategies.items()}\n",
        "    best_strat = max(sharpes, key=sharpes.get)\n",
        "    return sharpes[best_strat], best_strat\n",
        "\n",
        "# Pattern extractor\n",
        "def extract_market_pattern(df_block):\n",
        "    return df_block[features].rolling(5).mean().dropna().mean().values\n",
        "\n",
        "# Scale data\n",
        "df_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df[features + [target_col]]), columns=features + [target_col])\n",
        "df_scaled['Date'] = df['Date']\n",
        "\n",
        "# Dynamisch Einheiten berechnen mit Mindestgr√∂√üe 30\n",
        "min_unit_size = 20\n",
        "total_len = len(df_scaled)\n",
        "max_units = total_len // min_unit_size\n",
        "unit_size = total_len // 80\n",
        "\n",
        "if unit_size < min_unit_size:\n",
        "    raise ValueError(f\"Nicht genug Daten. Mindestens {min_unit_size * 80} ben√∂tigt.\")\n",
        "\n",
        "train_units = 60\n",
        "model_store = []\n",
        "\n",
        "print(\"\\nüìä Training Phase: 100 Units\")\n",
        "for i in range(train_units):\n",
        "    df_unit = df_scaled.iloc[i * unit_size:(i + 1) * unit_size].copy()\n",
        "    if len(df_unit) < min_unit_size:\n",
        "        continue\n",
        "\n",
        "    best_model, best_config, best_sharpe, best_corr, best_strat = None, None, -np.inf, None, None\n",
        "    for config in random.sample(hyperparam_space, 4):\n",
        "        input_width, units, drops, dense = config\n",
        "        df_b = df_unit.drop(columns=['Date'])\n",
        "        window = WindowGenerator(input_width, 1, 1, features, [target_col], df_b)\n",
        "        ds = window.make_dataset(df_b)\n",
        "\n",
        "        model = Sequential([\n",
        "            LSTM(units=units[0], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[0]),\n",
        "            LSTM(units=units[1], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[1]),\n",
        "            LSTM(units=units[2], return_sequences=False),\n",
        "            BatchNormalization(),\n",
        "            Dense(dense, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='mse')\n",
        "        model.fit(ds, validation_data=ds, epochs=20,\n",
        "                  callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)], verbose=0)\n",
        "\n",
        "        y_pred = model.predict(ds)\n",
        "        y_true = np.concatenate([y for x, y in ds], axis=0)\n",
        "\n",
        "        if y_pred.ndim == 3:\n",
        "            y_pred = y_pred[:, -1, :]\n",
        "        if y_true.ndim == 3:\n",
        "            y_true = y_true[:, -1, :]\n",
        "\n",
        "        corr, _ = pearsonr(np.ravel(y_true), np.ravel(y_pred))\n",
        "        sharpe, strat = evaluate_strategy(y_true, y_pred)\n",
        "\n",
        "        if sharpe > best_sharpe:\n",
        "            best_model, best_config, best_sharpe, best_corr, best_strat = model, config, sharpe, corr, strat\n",
        "\n",
        "    if best_model:\n",
        "        pattern_vector = extract_market_pattern(df_unit.drop(columns=['Date']))\n",
        "        model_store.append({\n",
        "            'unit': i,\n",
        "            'model': best_model,\n",
        "            'pattern': pattern_vector,\n",
        "            'config': best_config,\n",
        "            'strategy': best_strat\n",
        "        })\n",
        "        print(f\"‚úÖ Unit {i + 1}: Sharpe={best_sharpe:.3f} | Corr={best_corr:.3f} | Strategy={best_strat} | Config={best_config}\")\n",
        "\n",
        "print(f\"\\nüì¶ Anzahl gespeicherter Modelle: {len(model_store)}\")\n",
        "\n",
        "# Test Phase\n",
        "print(\"\\nüß™ Test Phase: Matching Models to Segments\")\n",
        "test_df = df_scaled.iloc[train_units * unit_size:].reset_index(drop=True)\n",
        "segment_size = unit_size\n",
        "for start in range(60, len(test_df) - segment_size, segment_size):\n",
        "    past_segment = test_df.iloc[start - 60:start]\n",
        "    future_segment = test_df.iloc[start:start + segment_size]\n",
        "    if len(future_segment) < segment_size:\n",
        "        break\n",
        "\n",
        "    pattern = extract_market_pattern(past_segment.drop(columns=['Date']))\n",
        "    stored_patterns = np.array([m['pattern'] for m in model_store if m['pattern'].shape == pattern.shape])\n",
        "    if len(stored_patterns) == 0:\n",
        "        print(\"‚ö†Ô∏è Kein gespeichertes Muster verf√ºgbar f√ºr den aktuellen Block.\")\n",
        "        continue\n",
        "\n",
        "    similarities = cosine_similarity([pattern], stored_patterns)[0]\n",
        "    best_index = np.argmax(similarities)\n",
        "    best_model_info = model_store[best_index]\n",
        "    best_model = best_model_info['model']\n",
        "\n",
        "    window = WindowGenerator(20, 1, 1, features, [target_col], future_segment.drop(columns=['Date']))\n",
        "    ds = window.make_dataset(future_segment.drop(columns=['Date']))\n",
        "\n",
        "    y_pred = best_model.predict(ds)\n",
        "    y_true = np.concatenate([y for x, y in ds], axis=0)\n",
        "    if y_pred.ndim == 3:\n",
        "        y_pred = y_pred[:, -1, :]\n",
        "    if y_true.ndim == 3:\n",
        "        y_true = y_true[:, -1, :]\n",
        "\n",
        "    sharpe, strat_used = evaluate_strategy(y_true, y_pred)\n",
        "    print(f\"üìà Segment ab {future_segment['Date'].iloc[0].date()} ‚ûú Strategy: {strat_used} | Sharpe={sharpe:.3f} | Source Unit: {best_model_info['unit'] + 1}\")"
      ],
      "metadata": {
        "id": "2n17syymSTVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Test Phase: Matching Models to Segments\n",
        "1/1 [==============================] - 0s 76ms/step\n",
        "üìà Segment ab 2015-11-15 ‚ûú Strategy: sign | Sharpe=0.164 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 72ms/step\n",
        "üìà Segment ab 2016-04-24 ‚ûú Strategy: adaptive | Sharpe=1.841 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 62ms/step\n",
        "üìà Segment ab 2016-10-02 ‚ûú Strategy: adaptive | Sharpe=19.665 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 61ms/step\n",
        "üìà Segment ab 2017-03-12 ‚ûú Strategy: adaptive | Sharpe=0.917 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 64ms/step\n",
        "üìà Segment ab 2017-08-20 ‚ûú Strategy: sign | Sharpe=4.686 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2018-01-28 ‚ûú Strategy: adaptive | Sharpe=6.371 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 67ms/step\n",
        "üìà Segment ab 2018-07-08 ‚ûú Strategy: adaptive | Sharpe=0.234 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 62ms/step\n",
        "üìà Segment ab 2018-12-16 ‚ûú Strategy: sign | Sharpe=1.908 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 66ms/step\n",
        "üìà Segment ab 2019-05-26 ‚ûú Strategy: adaptive | Sharpe=5.176 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 60ms/step\n",
        "üìà Segment ab 2019-11-03 ‚ûú Strategy: adaptive | Sharpe=0.964 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2020-04-12 ‚ûú Strategy: sign | Sharpe=90.423 | Source Unit: 39\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2020-09-20 ‚ûú Strategy: adaptive | Sharpe=0.715 | Source Unit: 48\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2021-02-28 ‚ûú Strategy: sign | Sharpe=0.675 | Source Unit: 48\n",
        "1/1 [==============================] - 0s 62ms/step\n",
        "üìà Segment ab 2021-08-08 ‚ûú Strategy: sign | Sharpe=0.540 | Source Unit: 47\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2022-01-16 ‚ûú Strategy: sign | Sharpe=18.160 | Source Unit: 47\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2022-06-26 ‚ûú Strategy: sign | Sharpe=0.273 | Source Unit: 51\n",
        "1/1 [==============================] - 0s 70ms/step\n",
        "üìà Segment ab 2022-12-04 ‚ûú Strategy: sign | Sharpe=26.019 | Source Unit: 41\n",
        "1/1 [==============================] - 0s 60ms/step\n",
        "üìà Segment ab 2023-05-14 ‚ûú Strategy: sign | Sharpe=12.706 | Source Unit: 41\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "üìà Segment ab 2023-10-22 ‚ûú Strategy: adaptive | Sharpe=0.885 | Source Unit: 41\n"
      ],
      "metadata": {
        "id": "g5gMYefH85WR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}